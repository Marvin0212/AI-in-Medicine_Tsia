# -*- coding: utf-8 -*-
"""predict_pretrained_std-1&1_3000+fft_maxpool.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Uc7HDyQ0eET3aOyk6EYQSzZN-YE-DBz

Import der benötigten Module. Das Modul "ecgdetectors" wird zuerst heruntergeladen und installiert.
"""

pip install keras-tuner

#import keras_tuner as kt
from google.colab import files
import csv
import scipy.io as sio
import matplotlib.pyplot as plt
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from scipy.signal import butter, filtfilt
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import resample
import pickle
from sklearn.preprocessing import normalize
from keras.utils import np_utils
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
import tensorflow as tf
from sklearn.utils import class_weight
import seaborn as sns
import os
import pickle
import json
import tempfile
import gc
nb_classes = 2

"""Upload-Dialog, um die EKG-Daten in die virtuelle Maschine zu laden und zu entpacken (es erfolgt keine Prüfung, ob die richtige Datei hochgeladen wurde)"""

uploaded = files.upload()
!unzip training.zip

"""Alternative: direkter Download aus Google Drive (deutlich schneller)"""

from google.colab import drive
drive.mount('/content/drive')
!cp "/content/drive/My Drive/Traindata_std_Low2.7/X_train_std.pkl" .    #Training and Validation Data
!cp "/content/drive/My Drive/Traindata_std_Low2.7/X_val_std.pkl" .
!cp "/content/drive/My Drive/Traindata_std_Low2.7/Y_train.pkl" .
!cp "/content/drive/My Drive/Traindata_std_Low2.7/Y_val.pkl" .
!cp "/content/drive/My Drive/Traindata_std_Low2.7/X_train_fft.pkl" .
!cp "/content/drive/My Drive/Traindata_std_Low2.7/X_val_fft.pkl" .
# !cp "/content/drive/My Drive/sdnn_afib.pkl" .                         #Raw ECG Data
# !cp "/content/drive/My Drive/sdnn_normal.pkl" .
!cp "/content/drive/My Drive/pred_train_Time.pkl" .                     #Stack Model Data
!cp "/content/drive/My Drive/pred_val_Time.pkl" .
!cp "/content/drive/My Drive/pred_train_fft.pkl" .
!cp "/content/drive/My Drive/pred_val_fft.pkl" .

with open('X_train.pkl', 'rb') as f:
    X_train = pickle.load(f)
with open('X_val.pkl', 'rb') as f:
    X_val = pickle.load(f)

with open('X_train_std.pkl', 'rb') as f:
    X_train_std = pickle.load(f)
with open('X_val_std.pkl', 'rb') as f:
    X_val_std = pickle.load(f)
with open('Y_train.pkl', 'rb') as f:
    Y_train = pickle.load(f)
with open('Y_val.pkl', 'rb') as f:
    Y_val = pickle.load(f)
with open('X_train_fft.pkl', 'rb') as f:
    X_train_fft = pickle.load(f)
with open('X_val_fft.pkl', 'rb') as f:
    X_val_fft = pickle.load(f)

with open('pred_train_Time.pkl', 'rb') as f:
    pred_train_Time = pickle.load(f)
with open('pred_val_Time.pkl', 'rb') as f:
    pred_val_Time = pickle.load(f)
with open('pred_train_fft.pkl', 'rb') as f:
    pred_train_fft = pickle.load(f)
with open('pred_val_fft.pkl', 'rb') as f:
    pred_val_fft = pickle.load(f)
with open('Y_train.pkl', 'rb') as f:
    Y_train = pickle.load(f)
with open('Y_val.pkl', 'rb') as f:
    Y_val = pickle.load(f)

with open('sdnn_normal.pkl', 'rb') as f:
    sdnn_normal = pickle.load(f)
with open('sdnn_afib.pkl', 'rb') as f:
    sdnn_afib = pickle.load(f)

"""# Run to get ECG Data from Zip File into Numpy Array"""

def data_preprocessing_short(ecg_lead):
        desired_length=3000
        amount_data, remainder=divmod(ecg_lead.shape[0],desired_length)
        if remainder!=0:
          # Loop the remainder part until it has the desired length
          looped_remainder = np.tile(ecg_lead[desired_length*amount_data:], (desired_length // remainder + 1))[:desired_length]
          # Attach the looped remainder back to the ecg_lead
          ecg_array_processed = np.concatenate((ecg_lead[0:desired_length*amount_data], looped_remainder))
          amount_data+=1
          return ecg_array_processed,amount_data     
        else:
          return ecg_lead,amount_data

sdnn_normal = np.array([])                              # Initialisierung der Feature-Arrays
sdnn_afib = np.array([])
ecg_class = np.array([])
with open('training/REFERENCE.csv') as csv_file:      # Einlesen der Liste mit Dateinamen und Zuordnung
    csv_reader = csv.reader(csv_file, delimiter=',')
    normal_line_count = 0
    afib_line_count = 0
    desired_length=3000
    for row in csv_reader:
      data = sio.loadmat('training/'+row[0]+'.mat')   # Import der EKG-Dateien
      ecg_lead = data['val'][0]
      ecg_array_processed, amount_data= data_preprocessing_short(ecg_lead) #data preprocessing 0=fill with zero 1=fill with mean 2=loop
      amount_data=np.arange(0,amount_data)
      if row[1] != 'A':                                                   # includes N,O,~
        for i in amount_data: 
          if normal_line_count != 0:                                        #check if its the first time so that vstack can work properly
            sdnn_normal = np.vstack((sdnn_normal,ecg_array_processed[i*desired_length:(i+1)*desired_length]))     
          else:
            sdnn_normal=ecg_array_processed[0:desired_length]
          normal_line_count = normal_line_count + 1    
      if row[1]=='A':
        for i in amount_data:                                                     # Zuordnung zu "Vorhofflimmern"          
          if afib_line_count != 0:                                          #check if its the first time so that vstack can work properly
            sdnn_afib = np.vstack((sdnn_afib,ecg_array_processed[i*desired_length:(i+1)*desired_length]))     
          else:
            sdnn_afib=ecg_array_processed[0:desired_length]                              
          afib_line_count = afib_line_count + 1

with open('sdnn_normal.pkl', 'wb') as f:
    pickle.dump(sdnn_normal, f)
with open('sdnn_afib.pkl', 'wb') as f:
    pickle.dump(sdnn_afib, f)

"""# Data Preprocessing

Splitting and Balancing Data
"""

#split data for testing and training
X_train_normal, X_test_normal = train_test_split(sdnn_normal, test_size=0.2, random_state=4)
X_train_afib, X_test_afib = train_test_split(sdnn_afib, test_size=0.2, random_state=4)
del sdnn_normal,sdnn_afib
####Data Augmentation
stretch_min=1.1
stretch_max=2.4
augmentation_factor=6
#stretch normal ecg
X_train_normal_length=len(X_train_normal)
stretch = np.arange(stretch_min,stretch_max,0.1)
stretch_choices = np.random.choice(stretch, X_train_normal_length*augmentation_factor)
X_train_normal_stretch = np.empty((len(stretch_choices), 3000 ))
index = np.arange(len(stretch_choices))
for i in index:
    stretch_length = int(len(X_train_normal[i%X_train_normal_length,:]) * stretch_choices[i])
    positions=np.arange(len(X_train_normal[i%X_train_normal_length,:]))
    new_positions = np.linspace(0, len(X_train_normal[i%X_train_normal_length,:]), stretch_length)
    X_train_normal_stretch[i,:] = np.interp(new_positions, positions, X_train_normal[i%X_train_normal_length,:])[0:3000]
X_train_normal=X_train_normal_stretch
del X_train_normal_length, stretch, stretch_choices, positions, new_positions,X_train_normal_stretch
#sretch afib ecg
X_train_afib_length=len(X_train_afib)
stretch = np.arange(stretch_min,stretch_max,0.1)
stretch_choices = np.random.choice(stretch, X_train_afib_length*augmentation_factor)
X_train_afib_stretch = np.empty((len(stretch_choices), 3000 ))
index = np.arange(len(stretch_choices))
for i in index:
    stretch_length = int(len(X_train_afib[i%X_train_afib_length,:]) * stretch_choices[i])
    positions=np.arange(len(X_train_afib[i%X_train_afib_length,:]))
    new_positions = np.linspace(0, len(X_train_normal[i%X_train_afib_length,:]), stretch_length)
    X_train_afib_stretch[i,:] = np.interp(new_positions, positions, X_train_afib[i%X_train_afib_length,:])[0:3000]
X_train_afib=X_train_afib_stretch
del X_train_afib_length, stretch, stretch_choices, positions, new_positions,X_train_afib_stretch
del i

#balance the training dataset without getting test data into the training data 
ids = np.arange(len(X_train_afib))
choices = np.random.choice(ids, len(X_train_normal))
X_train_afib_balanced = X_train_afib[choices]

#balance the test dataset without getting test data into the training data 
ids = np.arange(len(X_test_afib))
choices = np.random.choice(ids, len(X_test_normal))
X_test_afib_balanced = X_test_afib[choices]

#train class
normal_class_train=np.zeros(X_train_normal.shape[0])       
normal_class_train.fill(0)
afib_class_train=np.zeros(X_train_afib_balanced.shape[0])
afib_class_train.fill(1)

#test class
normal_class_test=np.zeros(X_test_normal.shape[0])       
normal_class_test.fill(0)
afib_class_test=np.zeros(X_test_afib_balanced.shape[0])
afib_class_test.fill(1)

#combine train and test set
X_train=np.concatenate((X_train_normal, X_train_afib_balanced))
y_train=np.concatenate((normal_class_train, afib_class_train))
del X_train_normal, X_train_afib_balanced,normal_class_train, afib_class_train
X_val=np.concatenate((X_test_normal, X_test_afib_balanced))
y_val=np.concatenate((normal_class_test, afib_class_test))
del X_test_normal, X_test_afib_balanced,normal_class_test, afib_class_test
#shuffle train 
order = np.arange(len(X_train))
np.random.shuffle(order)
X_train = X_train[order]
y_train = y_train[order]
#shuffle test
order = np.arange(len(X_val))
np.random.shuffle(order)
X_val = X_val[order]
y_val = y_val[order]
#convert for NN
X_train = X_train.astype('float32')
X_val = X_val.astype('float32')
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_val = np_utils.to_categorical(y_val, nb_classes)

with open('X_train.pkl', 'wb') as f:
    pickle.dump(X_train, f)
with open('X_val.pkl', 'wb') as f:
    pickle.dump(X_val, f)

sampling_freq = 300 # Hz
fft_size = len(X_train[0])
# Create a pre-allocated numpy array to store the FFTs of the time series in X_train
X_train_fft = np.empty((len(X_train), fft_size // 2 ))
X_val_fft = np.empty((len(X_val), fft_size // 2 ))

index = []
# Iterate over each time series in X_train and compute its FFT
for i, series in enumerate(X_train):
    fft_results = np.fft.fft(X_train[i,:], fft_size)
    # Only save the positive half of the FFT results
    X_train_fft[i,:] = fft_results[:fft_size // 2]
    # Cumulatively sum
    cumulative_sum = np.cumsum(abs(X_train_fft[i,:]))
    # Find the threshold value at which 99% of the power has been accumulated
    threshold = 0.99 * sum(abs(X_train_fft[i,:]))
    # Find the index of the first value in the cumulative sum that is greater than the threshold
    index =np.append(index,np.argwhere(cumulative_sum > threshold).flatten()[0])
#calculate cutoff frequency at which 99% of information is contained
f_cutoff=sum(index)/len(index)
cutoff_frequency=int(f_cutoff*0.1)

del index,fft_results,cumulative_sum
# Iterate over each time series in X_val and compute its FFT
for i, series in enumerate(X_val):
    fft_results = np.fft.fft(X_val[i,:], fft_size)
    # Only save the positive half of the FFT results
    X_val_fft[i,:] = fft_results[:fft_size // 2]

del fft_results
# #Cut FFT at that frequency
# X_train_fft=X_train_fft[:,0:int(f_cutoff)]
# X_val_fft=X_val_fft[:,0:int(f_cutoff)]

def plot_fft(X_train):
    # Get the number of time series
    n_time_series = X_train.shape[0]
    # Create a figure with n_time_series subplots
    fig, axs = plt.subplots(n_time_series, figsize=(10, 20))
    # Plot each time series in a separate subplot
    for i in range(n_time_series):
        axs[i].plot(X_train[i])
    # Show the plots
    plt.show()

# Plot the time series
plot_fft(abs(X_train_fft[30:40,:]))

# # Plot a single time series
# # Calculate the frequency axis of the FFT
# freq_axis = np.fft.fftfreq(len(X_train[0]), d=1/sampling_freq)
# freq_axis = freq_axis[:int(f_cutoff)]

# plt.plot(freq_axis, abs(X_train_fft[35]))
# plt.xlabel('Frequency (Hz)')
# plt.ylabel('Amplitude ')
# plt.show()

X_trainplt=X_train
X_valplt=X_val

X_train=X_trainplt
X_val=X_valplt

"""# Data Transformation"""

# #Band Pass Flter
# def butterworth_filter(time_series, lowcut, highcut, fs, order=5):
#     nyq = 0.5 * fs
#     low = lowcut / nyq
#     high = highcut / nyq
#     b, a = butter(order, [low, high], btype='band')
#     filtered_time_series = filtfilt(b, a, time_series)
#     return filtered_time_series

# index=np.arange(0,X_train.shape[0])
# for i in index:
#   X_train[i,:] = butterworth_filter(X_train[i,:], lowcut=1.5, highcut=cutoff_frequency, fs=300, order=2)

# index=np.arange(0,X_val.shape[0])
# for i in index:
#   X_val[i,:] = butterworth_filter(X_val[i,:], lowcut=1.5, highcut=cutoff_frequency, fs=300, order=2)

#High Pass Filter
def butterworth_filter(time_series, highcut, fs, order=5):
    nyq = 0.5 * fs
    high = highcut / nyq
    b, a = butter(order, high, btype='high')
    filtered_time_series = filtfilt(b, a, time_series)
    return filtered_time_series

index=np.arange(0,X_train.shape[0])
for i in index:
  X_train[i,:] = butterworth_filter(X_train[i,:], highcut=2.7, fs=300, order=2)

index=np.arange(0,X_val.shape[0])
for i in index:
  X_val[i,:] = butterworth_filter(X_val[i,:], highcut=2.7, fs=300, order=2)

#clip to get rid of outliers
X_train = np.clip(X_train, -500, 500)
X_val = np.clip(X_val, -500, 500)

#standardize the data between -1 and 1
def standardize_data(X_train, X_val):
    scaler = MinMaxScaler(feature_range=(-1, 1))
    X_train_std = scaler.fit_transform(X_train)
    X_val_std = scaler.transform(X_val)
    return X_train_std, X_val_std
X_train_std, X_val_std = standardize_data(X_train, X_val)
del X_train, X_val
# #standardize with mean=0 and std=1
# std_slc = StandardScaler()
# std_slc.fit(X_train)
# X_train_std = std_slc.transform(X_train)
# X_val_std = std_slc.transform(X_val)

#save slc for predictions
#pickle.dump(std_slc, open('/content/drive/My Drive/scaler.pkl','wb'))

print('Training labels shape:', Y_train.shape)
print('Validation labels shape:', Y_val.shape)
#print('Test labels shape:', test_labels.shape)

print('Training features shape:', X_train_std.shape)
print('Validation features shape:', X_val_std.shape)
#print('Test features shape:', test_features.shape)

print('max value:',np.amax(X_train_std))
print('mean value:',np.mean(X_train_std))
print('min value:',np.amin(X_train_std))

#calculate the class imbalance
sum_class1=Y_train[0:Y_train.shape[0],0]==1
sum_class2=Y_train[0:Y_train.shape[0],1]==1
sum_class1=sum_class1.sum()
sum_class2=sum_class2.sum()
total=sum_class1+sum_class2
print('Training Data:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, sum_class2, 100 * sum_class2 / total))

#calculate the class imbalance
sum_class1=Y_val[0:Y_val.shape[0],0]==1
sum_class2=Y_val[0:Y_val.shape[0],1]==1
sum_class1=sum_class1.sum()
sum_class2=sum_class2.sum()
total=sum_class1+sum_class2

print('Validation Data:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, sum_class2, 100 * sum_class2 / total))

# #calculate weights if thats how you adjust learning process with imbalanced dataset  
# weight_for_0 = (1 / sum_class1) * (total / 2.0)
# weight_for_1 = (1 / sum_class2) * (total / 2.0)

# class_weight = {0: weight_for_0,
#                 1: weight_for_1
#                 }

# print('Weight for class 0: {:.2f}'.format(weight_for_0))
# print('Weight for class 1: {:.2f}'.format(weight_for_1))

"""Plot ECG Data"""

#use this Function to get overview over ECG Data Set
def plot_time_series(X_train):
    # Get the number of time series
    n_time_series = X_train.shape[0]
    # Create a figure with n_time_series subplots
    fig, axs = plt.subplots(n_time_series, figsize=(15, 150))
    # Plot each time series in a separate subplot
    for i in range(n_time_series):
        axs[i].plot(X_train[i])
        axs[i].set_title("Time Series {}".format(i))
    # Show the plots
    plt.show()

# Plot the time series
plot_time_series(X_train_std[100:200,:])

# #Use this to plot specific ECG Data to keep track of them during preprocessing
# def plot_time_series(X_train, indices):
#     # Get the number of time series
#     n_time_series = len(indices)
#     # Create a figure with n_time_series subplots
#     fig, axs = plt.subplots(n_time_series, figsize=(15, 10))
#     # Plot each time series in a separate subplot
#     for i, idx in enumerate(indices):
#         axs[i].plot(X_train[idx])
#         axs[i].set_title("Time Series {}".format(idx))
#     # Show the plots
#     plt.show()

# # usage example
# indices = [104, 130, 148, 161 , 179]
# plot_time_series(X_train_std, indices)

with open('/content/drive/My Drive/Traindata_std_Low2.7/X_train_std.pkl', 'wb') as f:
    pickle.dump(X_train_std, f)
with open('/content/drive/My Drive/Traindata_std_Low2.7/X_val_std.pkl', 'wb') as f:
    pickle.dump(X_val_std, f)
with open('/content/drive/My Drive/Traindata_std_Low2.7/Y_train.pkl', 'wb') as f:
    pickle.dump(Y_train, f)
with open('/content/drive/My Drive/Traindata_std_Low2.7/Y_val.pkl', 'wb') as f:
    pickle.dump(Y_val, f)
with open('/content/drive/My Drive/Traindata_std_Low2.7/X_train_fft.pkl', 'wb') as f:
    pickle.dump(X_train_fft, f)
with open('/content/drive/My Drive/Traindata_std_Low2.7/X_val_fft.pkl', 'wb') as f:
    pickle.dump(X_val_fft, f)

"""# Keras Tuner Hypermodel Training"""

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
      keras.metrics.BinaryAccuracy(name='binary_accuracy')
]

def model_builder(hp):
  # input layers for the predictions of the two base models
  input_Time = keras.layers.Input(shape=(2,), name='input_1')
  input_FFT = keras.layers.Input(shape=(2,), name='input_2')
  input_FFT_Time = keras.layers.Input(shape=(2,), name='input_3')

  # Concatenate the predictions
  merged = keras.layers.Concatenate()([input_Time, input_FFT,input_FFT_Time])

  # Add layers to the model
  dense_units_1 = hp.Int('dense_units_1', min_value=8, max_value=256, step=16)
  dense_reg_1 = hp.Choice('dense_reg_1', values=[1e-2, 1e-3, 1e-4]) 
  dense_batch_norm_1 = layers.BatchNormalization()(merged)
  dense_1 = keras.layers.Dense(units=dense_units_1, activation='relu',kernel_regularizer=regularizers.l2(dense_reg_1))(dense_batch_norm_1)
  hp_drop_1 = hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)
  dropout_1 = keras.layers.Dropout(hp_drop_1)(dense_1)

  dense_units_2 = hp.Int('dense_units_2', min_value=8, max_value=256, step=16)
  dense_reg_2 = hp.Choice('dense_reg_2', values=[1e-2, 1e-3, 1e-4]) 
  dense_batch_norm_2 = layers.BatchNormalization()(dropout_1)
  dense_2 = keras.layers.Dense(units=dense_units_1, activation='relu',kernel_regularizer=regularizers.l2(dense_reg_1))(dense_batch_norm_2)
  hp_drop_2 = hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)
  dropout_2 = keras.layers.Dropout(hp_drop_1)(dense_2)

  dense_units_3 = hp.Int('dense_units_3', min_value=8, max_value=256, step=16)
  dense_reg_3 = hp.Choice('dense_reg_3', values=[1e-2, 1e-3, 1e-4]) 
  dense_batch_norm_3 = layers.BatchNormalization()(dropout_2)
  dense_3 = keras.layers.Dense(units=dense_units_1, activation='relu',kernel_regularizer=regularizers.l2(dense_reg_1))(dense_batch_norm_3)
  hp_drop_3 = hp.Float('dropout_3', min_value=0.0, max_value=0.5, step=0.1)
  dropout_3 = keras.layers.Dropout(hp_drop_1)(dense_3)

  dense_units_4 = hp.Int('dense_units_3', min_value=8, max_value=256, step=16)
  dense_reg_4 = hp.Choice('dense_reg_3', values=[1e-2, 1e-3, 1e-4]) 
  dense_batch_norm_4 = layers.BatchNormalization()(dropout_3)
  dense_4 = keras.layers.Dense(units=dense_units_1, activation='relu',kernel_regularizer=regularizers.l2(dense_reg_1))(dense_batch_norm_4)
  hp_drop_4 = hp.Float('dropout_3', min_value=0.0, max_value=0.5, step=0.1)
  dropout_4 = keras.layers.Dropout(hp_drop_1)(dense_4)

  #dense layer with a sigmoid activation function
  output = keras.layers.Dense(2, activation='sigmoid')(dropout_4)

  #final model
  model = keras.Model(inputs=[input_Time, input_FFT,input_FFT_Time], outputs=output)

# ##FFT
#   model = keras.Sequential()
#   # #1
#   model.add(keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001), input_shape=[1500,1])) 
#   model.add(layers.BatchNormalization())
#   model.add(tf.keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))
#   model.add(keras.layers.MaxPool1D(pool_size=4))

#   for i in range(hp.Int('1st_layers', 1, 2)):
#     model.add(layers.BatchNormalization())
#     model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))

#   model.add(keras.layers.MaxPool1D(pool_size=6))

#   for i in range(hp.Int('2nd_layers', 1, 4)):
#     model.add(layers.BatchNormalization())
#     model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))

#   model.add(keras.layers.MaxPool1D(pool_size=6))

#   for i in range(hp.Int('3rd_layers', 1, 4)):
#     model.add(layers.BatchNormalization())
#     model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))

#   model.add(keras.layers.MaxPool1D(pool_size=6))


#   model.add(keras.layers.Flatten())

#   model.add(layers.BatchNormalization())
#   model.add(keras.layers.Dense(units=384, activation='relu',kernel_regularizer=regularizers.l2(0.01), input_shape=[128]))
#   model.add(keras.layers.Dropout(0.3))

#   model.add(layers.BatchNormalization())
#   model.add(keras.layers.Dense(units=384, activation='relu',kernel_regularizer=regularizers.l2(0.01)))
#   model.add(keras.layers.Dropout(0.2))

#   model.add(layers.BatchNormalization())
#   model.add(keras.layers.Dense(2, activation='sigmoid'))
# ##FFT+Timeseries
#   input_1 = keras.layers.Input(shape=[3000, 1])
#   ##strides=2, halfes the output shape
#   conv_1 = keras.layers.Conv1D(filters=16, kernel_size=3,activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001),name='conv1')(input_1)
#   batch_norm_1 = layers.BatchNormalization()(conv_1)
#   conv_2 = keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_1)
#   max_pool_1 = keras.layers.MaxPool1D(pool_size=4)(conv_2)

#   batch_norm_2 = layers.BatchNormalization()(max_pool_1)
#   conv_3 = keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_2)
#   batch_norm_3 = layers.BatchNormalization()(conv_3)
#   conv_4 = keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_3)
#   max_pool_2 = keras.layers.MaxPool1D(pool_size=4)(conv_4)

#   batch_norm_4 = layers.BatchNormalization()(max_pool_2)
#   conv_5 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_4)
#   batch_norm_5 = layers.BatchNormalization()(conv_5)
#   conv_6 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_5)
#   batch_norm_6 = layers.BatchNormalization()(conv_6)
#   conv_7 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_6)
#   batch_norm_7 = layers.BatchNormalization()(conv_7)
#   conv_8 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_7)
#   max_pool_3 = keras.layers.MaxPool1D(pool_size=4)(conv_8)

#   batch_norm_8 = layers.BatchNormalization()(max_pool_3)
#   conv_9 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_8)
#   batch_norm_9 = layers.BatchNormalization()(conv_9)
#   conv_10 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_9)
#   batch_norm_10 = layers.BatchNormalization()(conv_10)
#   conv_11 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_10)
#   batch_norm_11 = layers.BatchNormalization()(conv_11)
#   conv_12 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_11)
#   max_pool_4 = keras.layers.MaxPool1D(pool_size=4)(conv_12)

#   batch_norm_12 = layers.BatchNormalization()(max_pool_4)
#   conv_13 = keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_12)
#   batch_norm_13 = layers.BatchNormalization()(conv_13)
#   conv_14 = keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_13)
#   batch_norm_14 = layers.BatchNormalization()(conv_14)
#   conv_15 = keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_14)
#   batch_norm_15 = layers.BatchNormalization()(conv_15)
#   conv_16 = keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_15)
#   pooling = hp.Choice('pooling_last_layer', values=[4, 6])
#   max_pool_5 = keras.layers.MaxPool1D(pool_size=pooling)(conv_16)

# ####input fft X_train_fft
#   input_2 = keras.layers.Input(shape=[1500, 1])

#   fft_conv_1 = keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(input_2)
#   fft_batch_norm_1 = layers.BatchNormalization()(fft_conv_1)
#   fft_conv_2 = keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_1)
#   fft_max_pool_1 = keras.layers.MaxPool1D(pool_size=4)(fft_conv_2)

#   fft_batch_norm_2 = layers.BatchNormalization()(fft_max_pool_1)
#   fft_conv_3 = keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_2)
#   fft_max_pool_2 = keras.layers.MaxPool1D(pool_size=6)(fft_conv_3)

#   fft_batch_norm_3 = layers.BatchNormalization()(fft_max_pool_2)
#   fft_conv_4 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_3)
#   fft_batch_norm_4 = layers.BatchNormalization()(fft_conv_4)
#   fft_conv_5 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_4)
#   fft_max_pool_3 = keras.layers.MaxPool1D(pool_size=6)(fft_conv_5)

#   fft_batch_norm_5 = layers.BatchNormalization()(fft_max_pool_3)
#   fft_conv_6 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_5)
#   fft_batch_norm_6 = layers.BatchNormalization()(fft_conv_6)
#   fft_conv_7 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_6)
#   fft_max_pool_4 = keras.layers.MaxPool1D(pool_size=6)(fft_conv_7)

#   # Merge the outputs of the two convolutional layers
#   flat_max_pool_5 = keras.layers.Flatten()(max_pool_5)
#   flat_fft_max_pool_4 = keras.layers.Flatten()(fft_max_pool_4)
#   # Concatenate the flattened layers
#   flatten = keras.layers.Concatenate()([flat_max_pool_5, flat_fft_max_pool_4])

#   # Add the rest of the layers to the model
#   dense_units_1 = hp.Int('dense_units_1', min_value=64, max_value=512, step=64)
#   dense_reg_1 = hp.Choice('dense_reg_1', values=[1e-2, 1e-3, 1e-4]) 
#   dense_batch_norm_1 = layers.BatchNormalization()(flatten)
#   dense_1 = keras.layers.Dense(units=dense_units_1, activation='relu',kernel_regularizer=regularizers.l2(dense_reg_1))(dense_batch_norm_1)

#   hp_drop_1 = hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)
#   dropout_1 = keras.layers.Dropout(hp_drop_1)(dense_1)

#   dense_units_2 = hp.Int('dense_units_2', min_value=64, max_value=512, step=64)
#   dense_reg_2 = hp.Choice('dense_reg_2', values=[1e-2, 1e-3, 1e-4]) 
#   dense_batch_norm_2 = layers.BatchNormalization()(dropout_1)
#   dense_2 = keras.layers.Dense(units=dense_units_2, activation='relu',kernel_regularizer=regularizers.l2(dense_reg_2))(dense_batch_norm_2)

#   hp_drop_2 = hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)
#   dropout_2 = keras.layers.Dropout(hp_drop_2)(dense_2)
  
#   dense_batch_norm_3 = layers.BatchNormalization()(dropout_2)
#   output = keras.layers.Dense(2, activation='sigmoid')(dense_batch_norm_3)
  
#   # Create the model with two inputs and one output
#   model = keras.Model(inputs=[input_1, input_2], outputs=output)


####old model
  # model = keras.Sequential()
  # #1
  # # hp_units_0 = hp.Int('units_0', min_value=24, max_value=88, step=8)
  # # hp_reg_0 = hp.Choice('reg_0', values=[1e-3, 1e-4])
  # model.add(keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001), input_shape=[3000,1]))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(keras.layers.MaxPool1D(pool_size=4))
  # #2
  # # hp_units_1 = hp.Int('units_1', min_value=24, max_value=88, step=8)
  # # hp_reg_1 = hp.Choice('reg_1', values=[1e-3, 1e-4])
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(keras.layers.MaxPool1D(pool_size=4))
  
  # #3
  # # hp_units_2 = hp.Int('units_2', min_value=24, max_value=88, step=8)
  # # hp_reg_2 = hp.Choice('reg_2', values=[1e-3, 1e-4])
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(keras.layers.MaxPool1D(pool_size=4))
  # #4
  # # hp_units_3 = hp.Int('units_3', min_value=24, max_value=88, step=8)
  # # hp_reg_3 = hp.Choice('reg_3', values=[1e-3, 1e-4])
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(keras.layers.MaxPool1D(pool_size=4))
  # #5
  # # hp_units_4 = hp.Int('units_4', min_value=24, max_value=88, step=8)
  # # hp_reg_4 = hp.Choice('reg_4', values=[1e-3, 1e-4])
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)))                                      
  # model.add(keras.layers.MaxPool1D(pool_size=4))

  # model.add(layers.BatchNormalization())
  # dense_units_0 = hp.Int('dense_units_0', min_value=128, max_value=512, step=64)
  # dense_reg_0 = hp.Choice('dense_reg_0', values=[1e-2, 1e-3, 1e-4])
  # model.add(keras.layers.Dense(units=dense_units_0, activation='relu',kernel_regularizer=regularizers.l2(dense_reg_0), input_shape=[512]))
  # hp_drop_0 = hp.Float('dropout_0', min_value=0.0, max_value=0.5, step=0.1)
  # model.add(keras.layers.Dropout(hp_drop_0))

  # model.add(layers.BatchNormalization())
  # dense_units_1 = hp.Int('dense_units_1', min_value=128, max_value=512, step=64)
  # dense_reg_1 = hp.Choice('dense_reg_1', values=[1e-2, 1e-3, 1e-4]) 
  # model.add(keras.layers.Dense(units=dense_units_1, activation='relu',kernel_regularizer=regularizers.l2(dense_reg_1)))
  # hp_drop_1 = hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)
  # model.add(keras.layers.Dropout(hp_drop_1))

  # model.add(layers.BatchNormalization())
  # model.add(keras.layers.Dense(2, activation='sigmoid'))


  # Initialize the Sequential API and start stacking the layers
  # model = keras.Sequential()
  # # Tune the number of units in the first Dense layer
  # # Choose an optimal value between 32-512
  # hp_units_0 = hp.Int('units_0', min_value=320, max_value=512, step=32)
  # hp_reg_0 = hp.Choice('reg_0', values=[1e-2, 1e-3, 1e-4])
  # model.add(keras.layers.Dense(units=hp_units_0, activation='relu',kernel_regularizer=regularizers.l2(hp_reg_0), name='dense_first', input_shape=[9000]))

  # model.add(layers.BatchNormalization())

  # hp_drop = hp.Float('dropout_first', min_value=0.0, max_value=0.7, step=0.1)
  # model.add(keras.layers.Dropout(hp_drop))

  # #second dense layer
  # hp_units_2 = hp.Int('units_2', min_value=320, max_value=512, step=32)
  # hp_reg_2 = hp.Choice('reg_2', values=[1e-2, 1e-3, 1e-4])
  # model.add(keras.layers.Dense(units=hp_units_2, activation='relu',kernel_regularizer=regularizers.l2(hp_reg_2), name='dense_2'))

  # model.add(layers.BatchNormalization())

  # hp_drop_2 = hp.Float('dropout_2', min_value=0.1, max_value=0.7, step=0.1)
  # model.add(keras.layers.Dropout(hp_drop_2))

  # #third dense layer
  # hp_units_3 = hp.Int('units_3', min_value=320, max_value=512, step=32)
  # hp_reg_3 = hp.Choice('reg_3', values=[1e-2, 1e-3, 1e-4])
  # model.add(keras.layers.Dense(units=hp_units_2, activation='relu',kernel_regularizer=regularizers.l2(hp_reg_2), name='dense_3'))

  # model.add(layers.BatchNormalization())

  # hp_drop_3 = hp.Float('dropout_3', min_value=0.1, max_value=0.7, step=0.1)
  # model.add(keras.layers.Dropout(hp_drop_3))
  # ##dense layer
  # hp_units_3 = hp.Int('units_3', min_value=32, max_value=512, step=32)
  # model.add(keras.layers.Dense(units=hp_units_3, activation='relu', name='dense_3'))
  # hp_drop_3 = hp.Float('dropout_3', min_value=0.1, max_value=0.7, step=0.1)
  # model.add(keras.layers.Dropout(hp_drop_3))
  # model.add(layers.BatchNormalization())

  # for i in range(hp.Int('layers', 2, 6)):
  #     model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i), 32, 512, step=32),
  #                                   activation='relu'))
  #     model.add(keras.layers.Dropout(hp.Float('dropout_'+ str(i), min_value=0.0, max_value=0.5, step=0.1)))
  #     model.add(layers.BatchNormalization())

  # model.add(keras.layers.Dense(2, activation='sigmoid'))
  # Tune the learning rate for the optimizer
  # Choose an optimal value from 0.01, 0.001, or 0.0001
  #########################
  #   model = keras.Sequential([

  #     # First Convolutional Block
  #     layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.01),  
  #                   # give the input dimensions in the first layer
  #                   input_shape=[9000,1]),                                       
  #     layers.MaxPool1D(),#4500,16

  #     # Second Convolutional Block
  #     layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.01)),
  #     layers.BatchNormalization(),
  #     layers.MaxPool1D(),#2250,32

  #     # Third Convolutional Block
  #     layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.01)),
  #     layers.BatchNormalization(),
  #     layers.MaxPool1D(),#1125,64

  #     # Forth Convolutional Block
  #     layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.01)),
  #     layers.BatchNormalization(),
  #     layers.MaxPool1D(),#563,64

  #     # Forth Convolutional Block
  #     layers.Conv1D(filters=8, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.01)),
  #     layers.BatchNormalization(),
  #     layers.MaxPool1D(),#282,2

  #     # Forth Convolutional Block
  #     layers.Conv1D(filters=8, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.01)),
  #     layers.BatchNormalization(),
  #     layers.MaxPool1D(),#141,2

  #     # Forth Convolutional Block
  #     layers.Conv1D(filters=4, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.01)),
  #     layers.BatchNormalization(),
  #     layers.MaxPool1D(),#71,2
  #     # Forth Convolutional Block
  #     layers.Conv1D(filters=2, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.01)),
  #     layers.BatchNormalization(),
  #     layers.MaxPool1D(),#71,2

  #     # Classifier Head
  #     layers.Flatten(),
  #     layers.Dense(16, activation='relu', input_shape=[70],kernel_regularizer=regularizers.l2(0.01)),                   #dense layer sizes could probably be chosen better
  #     layers.BatchNormalization(),
  #     layers.Dropout(0.5),
  #     layers.Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.01)),    
  #     layers.BatchNormalization(),
  #     layers.Dropout(0.5),
  #     layers.Dense(2, activation='sigmoid'),
  # ])
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                loss='binary_crossentropy',
                metrics=METRICS)
  return model

# Instantiate the tuner
#tuner.objective("prc", direction="max")
tuner = kt.Hyperband(model_builder, # the hypermodel
                     objective=kt.Objective('val_binary_accuracy', direction='max'),
                     max_epochs=200,
                     factor=3, # factor which you have seen above 
                     directory='dir', # directory to save logs 
                     project_name='khyperband'
                     )
tuner.overwrite=True
# hypertuning settings
tuner.search_space_summary()

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf dir

"""if you want to continue from a previous tuner search"""

# Continue the tuning process from the last completed trial
with open('tuner_summary.json', 'r') as f:
  summary = json.load(f)

"""Start Keras Tuner Search"""

#save progress after every 10 trials
# class SaveSummary(tf.keras.callbacks.Callback):
#   def on_epoch_end(self, epoch, logs=None):
#     # Save the summary only after every 10 trials
#     # if epoch % 10 == 0:
#     summary = tuner.results_summary()
#     print(summary,'content loaded into summary')
#     # Save the progress of the Hyperband object
#     with open('/content/drive/My Drive/tuner_summary.json', 'w') as f:
#       json.dump(summary, f)
#       print('json_dump')

#stop trial if there is no progress
stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

callbacks = [stop_early]#,SaveSummary()]

# Perform hypertuning
# tuner.search((X_train_std,X_train_fft), 
#              Y_train, epochs=50, 
#              validation_data=((X_val_std,X_val_fft), Y_val), 
#              batch_size=128,
#              #callbacks=callbacks
#              #,class_weight=class_weight
#              #,initial_trials=summary
#              )
# tuner.search(X_train_fft, 
#              Y_train, epochs=30, 
#              validation_data=(X_val_fft, Y_val), 
#              batch_size=128,
#              #callbacks=callbacks
#              #,class_weight=class_weight
#              #,initial_trials=summary
#              )

tuner.search(
    (pred_train_Time,pred_train_FFT,pred_train_FFT_Time), Y_train[:50000,:],
    validation_data=((pred_val_Time,pred_val_FFT,pred_val_FFT_Time), Y_val),
    epochs=500,
    batch_size=128,
    verbose=1,
)
tuner.results_summary()

"""# Visualizing and Building the Best Keras Model"""

# Build the model with the optimal hyperparameters
best_hp=tuner.get_best_hyperparameters()[0]
#print(best_hp.get('units'))
h_model = tuner.hypermodel.build(best_hp)
h_model.summary()
history=h_model.fit(X_train_std, Y_train, epochs=100, validation_data=(X_val_std, Y_val)
                    #,class_weight=class_weight
                    )

# #determine best epoch
# val_acc_per_epoch = history.history['val_recall']
# best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
# print('Best epoch: %d' % (best_epoch,))

print(Y_val)
print(Y_val.argmax(axis=1))
test_predictions_baseline = h_model.predict(X_val_std,verbose=0)
print(test_predictions_baseline)
print(test_predictions_baseline.argmax(axis=1))

#plot loss
history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();

#Confusion Matrix
train_predictions_baseline = h_model.predict(X_train_std,verbose=0)
test_predictions_baseline = h_model.predict(X_val_std,verbose=0)

def plot_cm(labels, predictions, p=0.5):
  cm = confusion_matrix(labels, predictions > p)
  plt.figure(figsize=(5,5))
  sns.heatmap(cm, annot=True, fmt="d")
  plt.title('Confusion matrix @{:.2f}'.format(p))
  plt.ylabel('Actual label')
  plt.xlabel('Predicted label')

  print('Normal ECG Detected (True Negatives): ', cm[0][0])
  print('Normal ECG Incorrectly Detected (False Positives): ', cm[0][1])
  print('AFIB ECG Missed (False Negatives): ', cm[1][0])
  print('AFIB ECG Detected (True Positives): ', cm[1][1])
  print('Total ECG AFIB: ', np.sum(cm[1]))

baseline_results = h_model.evaluate(X_val_std, Y_val,verbose=0)
for name, value in zip(h_model.metrics_names, baseline_results):
  print(name, ': ', value)
print()

plot_cm(Y_val.argmax(axis=1), test_predictions_baseline.argmax(axis=1))

#plot_cm(Y_train.argmax(axis=1), train_predictions_baseline.argmax(axis=1))

# #determine best epoch
val_acc_per_epoch = history.history['val_loss']
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

"""# Model with stride < 1 instead of Maxpooling Layer to increase Field of View"""

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
      keras.metrics.BinaryAccuracy(name='binary_accuracy')
]
def make_model_stride(metrics=METRICS, output_bias=None):
####input time series X_train
  input_1 = keras.layers.Input(shape=[3000, 1])
  #strides=2, halfes the output shape
  conv_1 = keras.layers.Conv1D(filters=16, kernel_size=3, strides=2,activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(input_1)
  batch_norm_1 = layers.BatchNormalization()(conv_1)
  conv_2 = keras.layers.Conv1D(filters=16, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_1)

  batch_norm_2 = layers.BatchNormalization()(conv_2)
  padding_1 = keras.layers.ZeroPadding1D(padding=375)(batch_norm_2)
  conv_3 = keras.layers.Conv1D(filters=32, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(padding_1)
  batch_norm_3 = layers.BatchNormalization()(conv_3)
  conv_4 = keras.layers.Conv1D(filters=32, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_3)

  batch_norm_4 = layers.BatchNormalization()(conv_4)
  padding_2 = keras.layers.ZeroPadding1D(padding=187)(batch_norm_4)
  conv_5 = keras.layers.Conv1D(filters=64, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(padding_2)
  batch_norm_5 = layers.BatchNormalization()(conv_5)
  conv_6 = keras.layers.Conv1D(filters=64, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_5)
  batch_norm_6 = layers.BatchNormalization()(conv_6)
  conv_7 = keras.layers.Conv1D(filters=64, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_6)
  batch_norm_7 = layers.BatchNormalization()(conv_7)
  conv_8 = keras.layers.Conv1D(filters=64, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_7)

  batch_norm_8 = layers.BatchNormalization()(conv_8)
  padding_3 = keras.layers.ZeroPadding1D(padding=23)(batch_norm_8)
  conv_9 = keras.layers.Conv1D(filters=128, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(padding_3)
  batch_norm_9 = layers.BatchNormalization()(conv_9)
  conv_10 = keras.layers.Conv1D(filters=128, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_9)
  batch_norm_10 = layers.BatchNormalization()(conv_10)
  conv_11 = keras.layers.Conv1D(filters=128, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_10)
  batch_norm_11 = layers.BatchNormalization()(conv_11)
  conv_12 = keras.layers.Conv1D(filters=128, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_11)

  batch_norm_12 = layers.BatchNormalization()(conv_12)
  padding_4 = keras.layers.ZeroPadding1D(padding=3)(batch_norm_12)
  conv_13 = keras.layers.Conv1D(filters=256, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(padding_4)
  batch_norm_13 = layers.BatchNormalization()(conv_13)
  conv_14 = keras.layers.Conv1D(filters=256, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_13)
  batch_norm_14 = layers.BatchNormalization()(conv_14)
  conv_15 = keras.layers.Conv1D(filters=256, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_14)
  batch_norm_15 = layers.BatchNormalization()(conv_15)
  conv_16 = keras.layers.Conv1D(filters=256, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_15)

####input fft X_train_fft
  input_2 = keras.layers.Input(shape=[1017, 1])
  #strides=2, halfes the output shape
  fft_padding_1 = keras.layers.ZeroPadding1D(padding=524)(input_2)
  fft_conv_1 = keras.layers.Conv1D(filters=16, kernel_size=3, strides=2,activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_padding_1)
  fft_batch_norm_1 = layers.BatchNormalization()(fft_conv_1)
  fft_conv_2 = keras.layers.Conv1D(filters=16, kernel_size=3,strides=2,  activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_1)

  fft_batch_norm_2 = layers.BatchNormalization()(fft_conv_2)
  fft_padding_2 = keras.layers.ZeroPadding1D(padding=258)(fft_batch_norm_2)
  fft_conv_3 = keras.layers.Conv1D(filters=32, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_padding_2)
  fft_batch_norm_3 = layers.BatchNormalization()(fft_conv_3)
  fft_conv_4 = keras.layers.Conv1D(filters=32, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_3)

  fft_batch_norm_4 = layers.BatchNormalization()(fft_conv_4)
  fft_padding_3 = keras.layers.ZeroPadding1D(padding=129)(fft_batch_norm_4)
  fft_conv_5 = keras.layers.Conv1D(filters=64, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_padding_3)
  fft_batch_norm_5 = layers.BatchNormalization()(fft_conv_5)
  fft_conv_6 = keras.layers.Conv1D(filters=64, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_5)
  fft_batch_norm_6 = layers.BatchNormalization()(fft_conv_6)
  fft_conv_7 = keras.layers.Conv1D(filters=64, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_6)
  fft_batch_norm_7 = layers.BatchNormalization()(fft_conv_7)
  fft_conv_8 = keras.layers.Conv1D(filters=64, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_7)

  fft_batch_norm_8 = layers.BatchNormalization()(fft_conv_8)
  fft_padding_4 = keras.layers.ZeroPadding1D(padding=16)(fft_batch_norm_8)
  fft_conv_9 = keras.layers.Conv1D(filters=128, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_padding_4)
  fft_batch_norm_9 = layers.BatchNormalization()(fft_conv_9)
  fft_conv_10 = keras.layers.Conv1D(filters=128, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_9)
  fft_batch_norm_10 = layers.BatchNormalization()(fft_conv_10)
  fft_conv_11 = keras.layers.Conv1D(filters=128, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_10)
  fft_batch_norm_11 = layers.BatchNormalization()(fft_conv_11)
  fft_conv_12 = keras.layers.Conv1D(filters=128, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_11)

  fft_batch_norm_12 = layers.BatchNormalization()(fft_conv_12)
  fft_padding_5 = keras.layers.ZeroPadding1D(padding=2)(fft_batch_norm_12)
  fft_conv_13 = keras.layers.Conv1D(filters=256, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_padding_5)
  fft_batch_norm_13 = layers.BatchNormalization()(fft_conv_13)
  fft_conv_14 = keras.layers.Conv1D(filters=256, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_13)
  fft_batch_norm_14 = layers.BatchNormalization()(fft_conv_14)
  fft_conv_15 = keras.layers.Conv1D(filters=256, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_14)
  fft_batch_norm_15 = layers.BatchNormalization()(fft_conv_15)
  fft_conv_16 = keras.layers.Conv1D(filters=256, kernel_size=3,strides=2, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_15)

  # Merge the outputs of the two convolutional layers
  merged = keras.layers.Concatenate()([conv_16, fft_conv_16])
  flatten=keras.layers.Flatten()(merged)

  # flat_max_pool_3 = keras.layers.Flatten()(conv_16)
  # flat_fft_max_pool_3 = keras.layers.Flatten()(fft_conv_16)
  # #Concatenate the flattened layers
  # flatten = keras.layers.Concatenate()([flat_max_pool_3, flat_fft_max_pool_3])

  # Add the rest of the layers to the model
  dense_batch_norm_1 = layers.BatchNormalization()(flatten)
  dense_1 = keras.layers.Dense(units=192, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_1)
  dropout_1 = keras.layers.Dropout(0.4)(dense_1)

  dense_batch_norm_2 = layers.BatchNormalization()(dropout_1)
  dense_2 = keras.layers.Dense(units=256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_2)
  dropout_2 = keras.layers.Dropout(0.2)(dense_2)

  dense_batch_norm_3 = layers.BatchNormalization()(dropout_2)
  output = keras.layers.Dense(2, activation='sigmoid')(dense_batch_norm_3)
  
  # Create the model with two inputs and one output
  model = keras.Model(inputs=[input_1, input_2], outputs=output)

  model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-2),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=metrics)
  
  return model

"""# CNN Model for the Time Series"""

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
      keras.metrics.BinaryAccuracy(name='binary_accuracy')
]
################CNN_Timeseries###################
def make_model_Time(metrics=METRICS, output_bias=None):
##input time series X_train
  input_1 = keras.layers.Input(shape=[3000, 1])
  ##strides=2, halfes the output shape
  conv_1 = keras.layers.Conv1D(filters=16, kernel_size=3,activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001),name='conv1')(input_1)
  batch_norm_1 = layers.BatchNormalization()(conv_1)
  conv_2 = keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_1)
  max_pool_1 = keras.layers.MaxPool1D(pool_size=4)(conv_2)

  batch_norm_2 = layers.BatchNormalization()(max_pool_1)
  conv_3 = keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_2)
  batch_norm_3 = layers.BatchNormalization()(conv_3)
  conv_4 = keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_3)
  max_pool_2 = keras.layers.MaxPool1D(pool_size=4)(conv_4)

  batch_norm_4 = layers.BatchNormalization()(max_pool_2)
  conv_5 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_4)
  batch_norm_5 = layers.BatchNormalization()(conv_5)
  conv_6 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_5)
  batch_norm_6 = layers.BatchNormalization()(conv_6)
  conv_7 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_6)
  batch_norm_7 = layers.BatchNormalization()(conv_7)
  conv_8 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_7)
  max_pool_3 = keras.layers.MaxPool1D(pool_size=4)(conv_8)

  batch_norm_8 = layers.BatchNormalization()(max_pool_3)
  conv_9 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_8)
  batch_norm_9 = layers.BatchNormalization()(conv_9)
  conv_10 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_9)
  batch_norm_10 = layers.BatchNormalization()(conv_10)
  conv_11 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_10)
  batch_norm_11 = layers.BatchNormalization()(conv_11)
  conv_12 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_11)
  max_pool_4 = keras.layers.MaxPool1D(pool_size=4)(conv_12)

  batch_norm_12 = layers.BatchNormalization()(max_pool_4)
  conv_13 = keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_12)
  batch_norm_13 = layers.BatchNormalization()(conv_13)
  conv_14 = keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_13)
  batch_norm_14 = layers.BatchNormalization()(conv_14)
  conv_15 = keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_14)
  batch_norm_15 = layers.BatchNormalization()(conv_15)
  conv_16 = keras.layers.Conv1D(filters=256, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(batch_norm_15)
  max_pool_5 = keras.layers.MaxPool1D(pool_size=4)(conv_16)

  flatten=keras.layers.Flatten()(max_pool_5)
##########
  # Add the rest of the layers to the model
  dense_batch_norm_1 = layers.BatchNormalization()(flatten)
  dense_1 = keras.layers.Dense(units=384, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_1)
  dropout_1 = keras.layers.Dropout(0.2)(dense_1)

  dense_batch_norm_2 = layers.BatchNormalization()(dropout_1)
  dense_2 = keras.layers.Dense(units=384, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_2)
  dropout_2 = keras.layers.Dropout(0.3)(dense_2)

  dense_batch_norm_3 = layers.BatchNormalization()(dropout_2)
  output = keras.layers.Dense(2, activation='sigmoid')(dense_batch_norm_3)
  
  # Create the model with two inputs and one output
  #model = keras.Model(inputs=[input_1, input_2], outputs=output)
  model = keras.Model(inputs=[input_1], outputs=output)
  model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-2),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=metrics)
  
  return model

"""# CNN Model for the FFT"""

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
      keras.metrics.BinaryAccuracy(name='binary_accuracy')
]
def make_model_FFT(metrics=METRICS, output_bias=None):
###input fft X_train_fft
  input_1 = keras.layers.Input(shape=[1500, 1])

  fft_conv_1 = keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(input_1)
  fft_batch_norm_1 = layers.BatchNormalization()(fft_conv_1)
  fft_conv_2 = keras.layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_1)
  fft_max_pool_1 = keras.layers.MaxPool1D(pool_size=4)(fft_conv_2)

  fft_batch_norm_2 = layers.BatchNormalization()(fft_max_pool_1)
  fft_conv_3 = keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_2)
  fft_max_pool_2 = keras.layers.MaxPool1D(pool_size=4)(fft_conv_3)

  fft_batch_norm_3 = layers.BatchNormalization()(fft_max_pool_2)
  fft_conv_4 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_3)
  fft_batch_norm_4 = layers.BatchNormalization()(fft_conv_4)
  fft_conv_5 = keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_4)
  fft_max_pool_3 = keras.layers.MaxPool1D(pool_size=4)(fft_conv_5)

  fft_batch_norm_5 = layers.BatchNormalization()(fft_max_pool_3)
  fft_conv_6 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_5)
  fft_batch_norm_6 = layers.BatchNormalization()(fft_conv_6)
  fft_conv_7 = keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same', kernel_regularizer=regularizers.l2(0.0001))(fft_batch_norm_6)
  fft_max_pool_4 = keras.layers.MaxPool1D(pool_size=5)(fft_conv_7)

  flatten=keras.layers.Flatten()(fft_max_pool_4)

  # Add the rest of the layers to the model
  dense_batch_norm_1 = layers.BatchNormalization()(flatten)
  dense_1 = keras.layers.Dense(units=384, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_1)
  dropout_1 = keras.layers.Dropout(0.2)(dense_1)

  dense_batch_norm_2 = layers.BatchNormalization()(dropout_1)
  dense_2 = keras.layers.Dense(units=384, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_2)
  dropout_2 = keras.layers.Dropout(0.3)(dense_2)

  dense_batch_norm_3 = layers.BatchNormalization()(dropout_2)
  output = keras.layers.Dense(2, activation='sigmoid')(dense_batch_norm_3)
  
  # Create the model with two inputs and one output
  #model = keras.Model(inputs=[input_1, input_2], outputs=output)
  model = keras.Model(inputs=[input_1], outputs=output)
  model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-2),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=metrics)
  
  return model

"""# *Seqentiell Model and Autoencoder Modell structure*"""

#################sequentiell model#####################
# def make_model(metrics=METRICS, output_bias=None):
#   model = keras.Sequential([

#       # First Convolutional Block
#       layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001),  
#                     # give the input dimensions in the first layer
#                     input_shape=[1500,1]),   
#       layers.BatchNormalization(),
#       layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)),                                          
#       layers.MaxPool1D(pool_size=4),#750

#       # Second Convolutional Block
#       layers.BatchNormalization(),
#       layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)),
#       layers.MaxPool1D(pool_size=6),#187

#       # Third Convolutional Block
#       layers.BatchNormalization(),
#       layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)),
#       layers.BatchNormalization(),
#       layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)),
#       layers.MaxPool1D(pool_size=6),#46

#       # Forth Convolutional Block
#       layers.BatchNormalization(),
#       layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)),
#       layers.BatchNormalization(),
#       layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding='same',kernel_regularizer=regularizers.l2(0.0001)),
#       layers.MaxPool1D(pool_size=6),#11

#       # Classifier Head
#       layers.Flatten(),
#       layers.Dense(384, activation='relu', input_shape=[128],kernel_regularizer=regularizers.l2(0.01)),             
#       layers.BatchNormalization(),
#       layers.Dropout(0.2),
#       layers.Dense(384, activation='relu',kernel_regularizer=regularizers.l2(0.01)),    
#       layers.BatchNormalization(),
#       layers.Dropout(0.3),
#       layers.Dense(2, activation='sigmoid'),
#   ])

#   model.compile(
#       optimizer=keras.optimizers.Adam(learning_rate=1e-3),
#       loss=keras.losses.BinaryCrossentropy(),
#       metrics=metrics)

#   return model

##################Autoencoder#######################
# model = keras.Sequential([
#     #Encoder
#     # First Convolutional Block
#     layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same',   #9000,32
#                   input_shape=[9000,1]),                                        
#     layers.MaxPool1D(2,padding='same'),#4500,32
#     layers.BatchNormalization(),  
#     # Second Convolutional Block
#     layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same'),#4500,16
#     layers.MaxPool1D(2,padding='same'),#2250,16
#     layers.BatchNormalization(),  

#     # Third Convolutional Block
#     layers.Conv1D(filters=8, kernel_size=3, activation="relu", padding='same'),#2250,8
#     layers.MaxPool1D(2,padding='same'),#1125,8
#     layers.BatchNormalization(),  

#     #Decoder
#     layers.Conv1D(filters=8, kernel_size=3, activation="relu", padding='same'),#1125,8
#     layers.UpSampling1D(2),#2250,8
#     layers.BatchNormalization(),  

#     layers.Conv1D(filters=16, kernel_size=3, activation="relu", padding='same'),#2250,16
#     layers.UpSampling1D(2),#4500,16
#     layers.BatchNormalization(),  

#     layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding='same'),#4500,32
#     layers.UpSampling1D(2),#9000,32
#     layers.BatchNormalization(),  

#     layers.Conv1D(filters=1, kernel_size=3, activation="relu", padding='same'),#9000,1
# ])

# model.compile(
#     optimizer='adam',
#     loss='mean_squared_error',
#     metrics=['mean_absolute_percentage_error'],
# )

# history = model.fit(
#     X_train_std, X_train_std,
#     validation_data=(X_val_std, X_val_std),
#     epochs=30,
#     verbose=1,
# )

# history_frame = pd.DataFrame(history.history)
# history_frame.loc[:, ['loss', 'val_loss']].plot()
# history_frame.loc[:, ['mean_absolute_percentage_error', 'val_mean_absolute_percentage_error']].plot();

"""# Training of Neural Network"""

#model = make_model()
# Load the model from the saved checkpoint
#model = load_model('/content/drive/My Drive/1-1CNN16&256_FullFFT&Signalmaxpool_augmentation5_512I_Dense384R0.01D0.2&384R0.01D0.3_O0.01.h5')
#model.summary()
# ##Checkpoints
# #early stopping
# early_stopping = tf.keras.callbacks.EarlyStopping(
#     monitor='val_loss', 
#     verbose=1,
#     patience=20, # number of epochs that didnt bring an improvement 
#     mode='min',
#     restore_best_weights=True)
# #safe best model after every epoch
# checkpoint = ModelCheckpoint(filepath='/content/drive/My Drive/best_model.h5', 
#                              monitor='val_loss', 
#                              save_best_only=True, 
#                              verbose=1)
#fft+Time signal
# history = model.fit(
#     (X_train_std,X_train_fft), Y_train,
#     validation_data=((X_val_std,X_val_fft), Y_val),
#     epochs=50,
#     #callbacks=[early_stopping],
#     callbacks=[checkpoint],
#     batch_size=128,
#     #class_weight=class_weight,  
#     verbose=1,
# )
#fft or Time series
# history = model.fit(
#     X_train_std, Y_train,
#     validation_data=(X_val_std, Y_val),
#     epochs=50,
#     #callbacks=[early_stopping],
#     callbacks=[checkpoint],
#     batch_size=128,
#     #class_weight=class_weight,  
#     verbose=1,
# )
# Number of models to train
n_models = 15

for i in range(n_models):
    # Create a new subset of data
    X_subset, Y_subset = resample(X_train_std, Y_train)
    # Create a new neural network model
    model = make_model_Time()
    # Give the model a unique name
    #model.name = "model_" + str(i)
    # Define the checkpoint function
    filepath='/content/drive/My Drive/best_model_Time'+str(i+10)+'.h5'
    checkpoint = ModelCheckpoint(filepath, 
                             monitor='val_loss', 
                             save_best_only=True, 
                             verbose=1)
    # Train the model on the subset of data
    history = model.fit(X_subset, Y_subset, validation_data=(X_val_std, Y_val), epochs=50, batch_size=128, callbacks=[checkpoint], verbose=1)
    del model,X_subset,Y_subset
    gc.collect()

#colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
# def plot_loss(history, label, n):
#   # Use a log scale on y-axis to show the wide range of values.
#   plt.semilogy(history.epoch, history.history['loss'],
#                color=colors[n], label='Train ' + label)
#   plt.semilogy(history.epoch, history.history['val_loss'],
#                color=colors[n], label='Val ' + label,
#                linestyle="--")
#   plt.xlabel('Epoch')
#   plt.ylabel('Loss')
# plot_loss(history, "Zero Bias", 0)

# # #determine best epoch
# val_acc_per_epoch = history.history['val_binary_accuracy']
# best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
# print('Best epoch: %d' % (best_epoch,))
# #plot loss
# history_frame = pd.DataFrame(history.history)
# history_frame.loc[:, ['loss', 'val_loss']].plot()
# history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();

# # plot different metrics
# def plot_metrics(history):
#   metrics = ['loss', 'prc', 'precision', 'recall']
#   for n, metric in enumerate(metrics):
#     name = metric.replace("_"," ").capitalize()
#     plt.subplot(2,2,n+1)
#     plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')
#     plt.plot(history.epoch, history.history['val_'+metric],
#              color=colors[0], linestyle="--", label='Val')
#     plt.xlabel('Epoch')
#     plt.ylabel(name)
#     if metric == 'loss':
#       plt.ylim([0, plt.ylim()[1]])
#     elif metric == 'auc':
#       plt.ylim([0.8,1])
#     else:
#       plt.ylim([0,1])
#     plt.legend()

# plot_metrics(history)

"""# Stack Model

get predictions for each model and combine them to Training and Validation Data
"""

def predict_time(model_names):
    # list to store the predictions of the base models
    data_length=20000
    pred_train_Time = np.empty((data_length, 2*16 ))
    pred_val_Time = np.empty((len(X_val_std), 2*16 ))
    for i, model_name in enumerate(model_names):
        # load the model
        model = load_model('/content/drive/My Drive/'+ model_name)
        # get the predictions for the train set
        pred_train_Time[:,i*2:(i*2)+2]=model.predict(X_train_std[:20000,:])
        # get the predictions for the validation set
        pred_val_Time[:,i*2:(i*2)+2]=model.predict(X_val_std)
    return pred_train_Time, pred_val_Time
time_models = ['best_model_Time0.h5', 'best_model_Time1.h5', 'best_model_Time2.h5', 'best_model_Time3.h5',
               'best_model_Time4.h5', 'best_model_Time5.h5', 'best_model_Time6.h5', 'best_model_Time7.h5',
               'best_model_Time8.h5', 'best_model_Time9.h5', 'best_model_Time10.h5', 'best_model_Time11.h5',
               'best_model_Time12.h5','best_model_Time13.h5','best_model_Time14.h5', 'best_model_Time15.h5']
pred_train_Time, pred_val_Time = predict_time(time_models)

def predict_fft(model_names):
    data_length=20000
    pred_train_fft = np.empty((data_length, 2*16 ))
    pred_val_fft = np.empty((len(X_val_fft), 2*16 ))
    for i, model_name in enumerate(model_names):
        # load the model
        model = load_model('/content/drive/My Drive/'+model_name)
        # get the predictions for the train set
        pred_train_fft[:,i*2:(i*2)+2]=model.predict(X_train_fft[:20000,:])
        # get the predictions for the validation set
        pred_val_fft[:,i*2:(i*2)+2]=model.predict(X_val_fft)
    return pred_train_fft, pred_val_fft
fft_models = ['best_model_FFT0.h5', 'best_model_FFT1.h5', 'best_model_FFT2.h5', 'best_model_FFT3.h5',
               'best_model_FFT4.h5', 'best_model_FFT5.h5', 'best_model_FFT6.h5', 'best_model_FFT7.h5',
               'best_model_FFT8.h5', 'best_model_FFT9.h5', 'best_model_FFT10.h5', 'best_model_FFT11.h5',
               'best_model_FFT12.h5','best_model_FFT13.h5','best_model_FFT14.h5', 'best_model_FFT15.h5']
pred_train_fft, pred_val_fft = predict_fft(fft_models)

with open('/content/drive/My Drive/pred_train_Time.pkl', 'wb') as f:
    pickle.dump(pred_train_Time, f)
with open('/content/drive/My Drive/pred_val_Time.pkl', 'wb') as f:
    pickle.dump(pred_val_Time, f)
with open('/content/drive/My Drive/pred_train_fft.pkl', 'wb') as f:
    pickle.dump(pred_train_fft, f)
with open('/content/drive/My Drive/pred_val_fft.pkl', 'wb') as f:
    pickle.dump(pred_val_fft, f)

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
      keras.metrics.BinaryAccuracy(name='binary_accuracy')
]
def make_stack_model(metrics=METRICS, output_bias=None):
  # input layers for the predictions of the two base models
  input_Time = keras.layers.Input(shape=(32,1), name='input_1')
  input_FFT = keras.layers.Input(shape=(32,1), name='input_2')
  # Concatenate the predictions
  merged = keras.layers.Concatenate()([input_Time, input_FFT])
  flatten=keras.layers.Flatten()(merged)
  # Add layers to the model
  dense_batch_norm_1 = layers.BatchNormalization()(flatten)
  dense_1 = keras.layers.Dense(units=64, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_1)
  dropout_1 = keras.layers.Dropout(0.3)(dense_1)

  dense_batch_norm_2 = layers.BatchNormalization()(dropout_1)
  dense_2 = keras.layers.Dense(units=64, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_2)
  dropout_2 = keras.layers.Dropout(0.3)(dense_2)

  dense_batch_norm_3 = layers.BatchNormalization()(dropout_2)
  dense_3 = keras.layers.Dense(units=64, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense_batch_norm_3)
  dropout_3 = keras.layers.Dropout(0.3)(dense_2)

  #dense layer with a sigmoid activation function
  output = keras.layers.Dense(2, activation='sigmoid')(dropout_3)

  #final model
  model = keras.Model(inputs=[input_Time, input_FFT], outputs=output)
  model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-2),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=metrics)
  return model

stack_model = make_stack_model()
stack_model.summary()

#safe best model after every epoch
checkpoint = ModelCheckpoint(filepath='/content/drive/My Drive/best_stack_model.h5', 
                             monitor='val_binary_accuracy', 
                             save_best_only=True, 
                             verbose=1)
##fft+Time signal
history = stack_model.fit(
    (pred_train_Time,pred_train_fft), Y_train[:20000,:],
    validation_data=((pred_val_Time,pred_val_fft), Y_val),
    epochs=50,
    callbacks=[checkpoint],
    batch_size=128,
    verbose=1,
)

#plot loss
history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();

"""# Visualize Data"""

#get best model from Training
model=load_model('/content/drive/My Drive/best_stack_model0.93060.h5')
#predict CNN Model
#train_predictions_baseline = model.predict((X_train_std,X_train_fft),verbose=0)
#test_predictions_baseline = model.predict((X_val_std,X_val_fft))

#predict stack model
test_predictions_baseline = model.predict((pred_val_Time,pred_val_fft))
def plot_cm(labels, predictions, p=0.5):
  cm = confusion_matrix(labels, predictions > p)
  plt.figure(figsize=(5,5))
  sns.heatmap(cm, annot=True, fmt="d")
  plt.title('Confusion matrix @{:.2f}'.format(p))
  plt.ylabel('Actual label')
  plt.xlabel('Predicted label')

  print('Normal ECG Detected (True Negatives): ', cm[0][0])
  print('Normal ECG Incorrectly Detected (False Positives): ', cm[0][1])
  print('AFIB ECG Missed (False Negatives): ', cm[1][0])
  print('AFIB ECG Detected (True Positives): ', cm[1][1])
  print('Total ECG AFIB: ', np.sum(cm[1]))

#baseline_results = model.evaluate((X_val_std,X_val_fft), Y_val,verbose=0)
baseline_results = model.evaluate((pred_val_Time,pred_val_fft), Y_val)
for name, value in zip(model.metrics_names, baseline_results):
  print(name, ': ', value)
print()

plot_cm(Y_val.argmax(axis=1), test_predictions_baseline.argmax(axis=1))
#plot_cm(Y_train.argmax(axis=1), train_predictions_baseline.argmax(axis=1))

#plot loss
history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();

# #determine best epoch
val_acc_per_epoch = history.history['val_binary_accuracy']
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

"""Confusion Matrix"""

colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
def plot_roc(name, labels, predictions, **kwargs):
  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)

  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)
  plt.xlabel('False positives [%]')
  plt.ylabel('True positives [%]')
  plt.xlim([-0.5,20])
  plt.ylim([80,100.5])
  plt.grid(True)
  ax = plt.gca()
  ax.set_aspect('equal')
plot_roc("Train Baseline", Y_train.argmax(axis=1), train_predictions_baseline.argmax(axis=1), color=colors[0])
plot_roc("Test Baseline", Y_val.argmax(axis=1), test_predictions_baseline.argmax(axis=1), color=colors[0], linestyle='--')
plt.legend(loc='lower right');
plt.show()

def plot_prc(name, labels, predictions, **kwargs):
    precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)

    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)
    plt.xlabel('Precision')
    plt.ylabel('Recall')
    plt.grid(True)
    ax = plt.gca()
    ax.set_aspect('equal')
plot_prc("Train Baseline", Y_train.argmax(axis=1), train_predictions_baseline.argmax(axis=1), color=colors[0])
plot_prc("Test Baseline", Y_val.argmax(axis=1), test_predictions_baseline.argmax(axis=1), color=colors[0], linestyle='--')
plt.legend(loc='lower right');