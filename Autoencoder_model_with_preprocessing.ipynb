{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282kJTl05oyP"
      },
      "source": [
        "Import der benötigten Module. Das Modul \"ecgdetectors\" wird zuerst heruntergeladen und installiert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXBKNiHRpgrI"
      },
      "source": [
        "from google.colab import files\n",
        "import csv\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.utils import np_utils\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "nb_classes = 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKnd6OTz8El7"
      },
      "source": [
        "Upload-Dialog, um die EKG-Daten in die virtuelle Maschine zu laden und zu entpacken (es erfolgt keine Prüfung, ob die richtige Datei hochgeladen wurde)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB3LIxh3qR5y"
      },
      "source": [
        "uploaded = files.upload()\n",
        "!unzip training.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bT9C5tjIX-w"
      },
      "source": [
        "Alternative: direkter Download aus Google Drive (deutlich schneller)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCGYV2hWHuZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2162d38b-9f71-46d6-ba65-77e0687f928e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/training.zip\" .\n",
        "!unzip training.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  training.zip\n",
            "replace training/REFERENCE.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace training/train_ecg_00001.mat? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(ecg_lead,zero_mean_loop):\n",
        "  if zero_mean_loop ==2:# repeat array to fill \n",
        "        if ecg_lead.size<9000: \n",
        "          ecg_array_processed = np.array([])\n",
        "          ecg_array_processed_part2 = np.zeros((9000,), dtype=int)\n",
        "          #data gets looped till it has size of atleast 9000\n",
        "          repetitions,remainder=divmod(9000,ecg_lead.size)\n",
        "          repetitions+=1 \n",
        "          for x in range(repetitions): \n",
        "            ecg_array_processed=np.concatenate((ecg_array_processed,ecg_lead))\n",
        "          return ecg_array_processed[0:9000],ecg_array_processed_part2[0:9000]      \n",
        "        if ecg_lead.size>9000:\n",
        "          #first 9000 can be used unaltered\n",
        "          ecg_array_processed=ecg_lead[0:9000]\n",
        "          ecg_array_processed_part2 = np.array([])\n",
        "          #data above 9000 gets looped till it has size of atleast 9000\n",
        "          repetitions,remainder=divmod(9000,ecg_lead.size-9000)\n",
        "          repetitions+=1 \n",
        "          for x in range(repetitions): \n",
        "            ecg_array_processed_part2=np.concatenate((ecg_array_processed_part2,ecg_lead[9000:ecg_lead.size]))\n",
        "          return ecg_array_processed[0:9000],ecg_array_processed_part2[0:9000]\n",
        "        if ecg_lead.size==9000:\n",
        "          ecg_array_processed=ecg_lead[0:9000]\n",
        "          ecg_array_processed_part2 = np.zeros((9000,), dtype=int)\n",
        "          return ecg_array_processed,ecg_array_processed_part2[0:9000]\n",
        "  else: # zero or mean fill of array\n",
        "        if zero_mean_loop == 0:\n",
        "          ecg_array_processed = np.zeros((9000,), dtype=int)\n",
        "          ecg_array_processed_part2 = np.zeros((10000,), dtype=int)\n",
        "        if zero_mean_loop ==1:\n",
        "          ecg_mean=np.mean(ecg_lead)   \n",
        "          ecg_array_processed = np.empty([9000,])\n",
        "          ecg_array_processed_part2 = np.empty([10000,])\n",
        "          ecg_array_processed.fill(ecg_mean)\n",
        "          ecg_array_processed_part2.fill(ecg_mean)\n",
        "        if ecg_lead.size<9000:      \n",
        "          ecg_array_processed[0:ecg_lead.size]=ecg_lead[0:ecg_lead.size]\n",
        "          return ecg_array_processed,ecg_array_processed_part2[0:9000]      \n",
        "        if ecg_lead.size>9000:\n",
        "          ecg_array_processed=ecg_lead[0:9000]\n",
        "          ecg_array_processed_part2[0:ecg_lead.size-9000]=ecg_lead[9000:ecg_lead.size]\n",
        "          return ecg_array_processed,ecg_array_processed_part2[0:9000]\n",
        "        if ecg_lead.size==9000:\n",
        "          ecg_array_processed=ecg_lead[0:9000]\n",
        "          return ecg_array_processed,ecg_array_processed_part2[0:9000]"
      ],
      "metadata": {
        "id": "YmUUKFj1LzoW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdnn_normal = np.array([])                                # Initialisierung der Feature-Arrays\n",
        "sdnn_afib = np.array([])\n",
        "ecg_class = np.array([])\n",
        "with open('training/REFERENCE.csv') as csv_file:      # Einlesen der Liste mit Dateinamen und Zuordnung\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    normal_line_count = 0\n",
        "    afib_line_count = 0\n",
        "    for row in csv_reader:\n",
        "      data = sio.loadmat('training/'+row[0]+'.mat')   # Import der EKG-Dateien\n",
        "      ecg_lead = data['val'][0]\n",
        "      ecg_array_processed,ecg_array_processed_part2 = data_preprocessing(ecg_lead,2) #data preprocessing 0=fill with zero 1=fill with mean 2=loop\n",
        "      if row[1] != 'A':                                                   # includes N,O,~\n",
        "        if normal_line_count != 0:                                        #check if its the first time so that vstack can work properly\n",
        "          sdnn_normal = np.vstack((sdnn_normal,ecg_array_processed)) \n",
        "          if ecg_lead.size>9000:\n",
        "            sdnn_normal = np.vstack((sdnn_normal,ecg_array_processed_part2))       \n",
        "        else:\n",
        "          sdnn_normal=ecg_array_processed\n",
        "          if ecg_lead.size>9000:\n",
        "            sdnn_normal = np.vstack((sdnn_normal,ecg_array_processed_part2)) \n",
        "        normal_line_count = normal_line_count + 1    \n",
        "      if row[1]=='A':                                                     # Zuordnung zu \"Vorhofflimmern\"          \n",
        "        if afib_line_count != 0:                                          #check if its the first time so that vstack can work properly\n",
        "          sdnn_afib = np.vstack((sdnn_afib,ecg_array_processed))\n",
        "          if ecg_lead.size>9000:\n",
        "            sdnn_afib = np.vstack((sdnn_afib,ecg_array_processed_part2))        \n",
        "        else:\n",
        "          sdnn_afib=ecg_array_processed\n",
        "          if ecg_lead.size>9000:\n",
        "            sdnn_afib = np.vstack((sdnn_afib,ecg_array_processed_part2))                                 \n",
        "        afib_line_count = afib_line_count + 1\n",
        "\n",
        "    if (normal_line_count+ afib_line_count % 100)==0:\n",
        "        print(str(normal_line_count+ afib_line_count) + \"\\t Dateien wurden verarbeitet.\")"
      ],
      "metadata": {
        "id": "T4FNu8VQNw6t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.concatenate((sdnn_normal, sdnn_afib)) #combine Normal and Afib Data\n",
        "\n",
        "normal_class=np.zeros(sdnn_normal.shape[0])       #combine class labels\n",
        "normal_class.fill(0)\n",
        "afib_class=np.zeros(sdnn_afib.shape[0])\n",
        "afib_class.fill(1)\n",
        "y=np.concatenate((normal_class, afib_class))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)#split data for testing and training\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)"
      ],
      "metadata": {
        "id": "OyGneuzUS4fk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_slc = StandardScaler()\n",
        "std_slc.fit(X_train)\n",
        "X_train_std = std_slc.transform(X_train)\n",
        "X_test_std = std_slc.transform(X_test)"
      ],
      "metadata": {
        "id": "czwurA_tgZ_l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained_base = tf.keras.models.load_model(\n",
        "#     '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n",
        "# )\n",
        "# pretrained_base.trainable = False\n",
        "\n",
        "\n",
        "# model = keras.Sequential([\n",
        "#     pretrained_base,\n",
        "#     layers.Flatten(),\n",
        "#     layers.Dense(6, activation='relu'),\n",
        "#     layers.Dense(1, activation='sigmoid'),\n",
        "# ])\n",
        "#Dense Model\n",
        "# model = keras.Sequential([\n",
        "#     layers.Dense(4500, activation='relu', input_shape=[9000]),\n",
        "#     layers.Dropout(0.3),\n",
        "#     layers.BatchNormalization(),\n",
        "#     layers.Dense(2200, activation='relu'),  \n",
        "#     layers.Dropout(0.3),\n",
        "#     layers.BatchNormalization(), \n",
        "#     layers.Dense(1000, activation='relu'),  \n",
        "#     layers.Dropout(0.3),\n",
        "#     layers.BatchNormalization(), \n",
        "#     layers.Dense(500, activation='relu'),  \n",
        "#     layers.Dropout(0.3),\n",
        "#     layers.BatchNormalization(),\n",
        "#     layers.Dense(100, activation='relu'),  \n",
        "#     layers.Dropout(0.3),\n",
        "#     layers.BatchNormalization(),\n",
        "#     layers.Dense(10, activation='relu'),  \n",
        "#     layers.Dropout(0.3),\n",
        "#     layers.BatchNormalization(),\n",
        "#     layers.Dense(2, activation='sigmoid'),\n",
        "# ])\n",
        "\n",
        "#CNN\n",
        "# model = keras.Sequential([\n",
        "\n",
        "#     # First Convolutional Block\n",
        "#     layers.Conv1D(filters=10, kernel_size=5, activation=\"relu\", padding='same',   #the filter size i chose is random in each layer so far\n",
        "#                   # give the input dimensions in the first layer\n",
        "#                   input_shape=[9000,1]),                                        #i am a little confused about input shape and if thats right\n",
        "#     layers.MaxPool1D(),\n",
        "\n",
        "#     # Second Convolutional Block\n",
        "#     layers.Conv1D(filters=10, kernel_size=3, activation=\"relu\", padding='same'),\n",
        "#     layers.MaxPool1D(),\n",
        "\n",
        "#     # Third Convolutional Block\n",
        "#     layers.Conv1D(filters=10, kernel_size=3, activation=\"relu\", padding='same'),\n",
        "#     layers.MaxPool1D(),\n",
        "\n",
        "#     # Classifier Head\n",
        "#     layers.Flatten(),\n",
        "#     layers.Dense(5, activation='relu', input_shape=[9000]),                   #dense layer sizes could probably be chosen better\n",
        "#     layers.Dense(10, activation='relu'),    \n",
        "#     layers.Dense(2, activation='sigmoid'),\n",
        "# ])\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(9000, activation='relu', input_shape=[9000]),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(9000, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(9000, activation='relu'),\n",
        "])\n",
        "\n",
        "#Autoencoder\n",
        "# model = keras.Sequential([\n",
        "#     #Encoder\n",
        "#     # First Convolutional Block\n",
        "#     layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding='same',   #9000,32\n",
        "#                   input_shape=[9000,1]),                                        \n",
        "#     layers.MaxPool1D(2,padding='same'),#4500,32\n",
        "#     layers.BatchNormalization(),  \n",
        "#     # Second Convolutional Block\n",
        "#     layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\", padding='same'),#4500,16\n",
        "#     layers.MaxPool1D(2,padding='same'),#2250,16\n",
        "#     layers.BatchNormalization(),  \n",
        "\n",
        "#     # Third Convolutional Block\n",
        "#     layers.Conv1D(filters=8, kernel_size=3, activation=\"relu\", padding='same'),#2250,8\n",
        "#     layers.MaxPool1D(2,padding='same'),#1125,8\n",
        "#     layers.BatchNormalization(),  \n",
        "\n",
        "#     #Decoder\n",
        "#     layers.Conv1D(filters=8, kernel_size=3, activation=\"relu\", padding='same'),#1125,8\n",
        "#     layers.UpSampling1D(2),#2250,8\n",
        "#     layers.BatchNormalization(),  \n",
        "\n",
        "#     layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\", padding='same'),#2250,16\n",
        "#     layers.UpSampling1D(2),#4500,16\n",
        "#     layers.BatchNormalization(),  \n",
        "\n",
        "#     layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding='same'),#4500,32\n",
        "#     layers.UpSampling1D(2),#9000,32\n",
        "#     layers.BatchNormalization(),  \n",
        "\n",
        "#     layers.Conv1D(filters=1, kernel_size=3, activation=\"relu\", padding='same'),#9000,1\n",
        "# ])\n",
        "\n",
        "#####################Autoencoder#################################\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mean_squared_error',\n",
        "    metrics=['mean_absolute_percentage_error'],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_std, X_train_std,\n",
        "    validation_data=(X_test_std, X_test_std),\n",
        "    epochs=30,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "history_frame = pd.DataFrame(history.history)\n",
        "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
        "history_frame.loc[:, ['mean_absolute_percentage_error', 'val_mean_absolute_percentage_error']].plot();\n",
        "\n",
        "#################For CNN or Dense Model#########################\n",
        "# model.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss='binary_crossentropy',\n",
        "#     metrics=['binary_accuracy'],\n",
        "# )\n",
        "# history = model.fit(                      \n",
        "#     X_train_std, Y_train,\n",
        "#     validation_data=(X_test_std, Y_test),\n",
        "#     epochs=15,\n",
        "#     verbose=1,\n",
        "# )\n",
        "# history_frame = pd.DataFrame(history.history)\n",
        "# history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
        "# history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "_nSPJ83YcxWL",
        "outputId": "95317d7c-b54c-44dc-e0ee-878ab9c1279d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dd4f4707502e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# ])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m model = keras.Sequential([\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    }
  ]
}